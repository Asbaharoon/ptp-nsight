<!-- Creator     : groff version 1.19.2 -->
<!-- CreationDate: Fri Aug 26 10:45:13 2011 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p     { margin-top: 0; margin-bottom: 0; }
       pre   { margin-top: 0; margin-bottom: 0; }
       table { margin-top: 0; margin-bottom: 0; }
</style>
<title>MPI_Allreduce</title>

</head>
<body>

<h1 align=center>MPI_Allreduce</h1>



<a name="NAME"></a>
<h2>NAME</h2>



<p style="margin-left:11%; margin-top: 1em"><b>MPI_Allreduce</b>
&minus; Combines values from all processes and distributes
the result back to all processes.</p>

<a name="SYNTAX"></a>
<h2>SYNTAX</h2>


<a name="C Syntax"></a>
<h2>C Syntax</h2>


<p style="margin-left:11%; margin-top: 1em">#include
&lt;mpi.h&gt; <br>
int MPI_Allreduce(void <i>*sendbuf</i>, void
<i>*recvbuf</i>, int <i>count</i>,</p>

<table width="100%" border=0 rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="8%"></td>
<td width="7%"></td>
<td width="85%">


<p valign="top">MPI_Datatype <i>datatype</i>, MPI_Op
<i>op</i>, MPI_Comm <i>comm</i>)</p></td>
</table>

<a name="Fortran Syntax"></a>
<h2>Fortran Syntax</h2>


<p style="margin-left:11%; margin-top: 1em">INCLUDE
&rsquo;mpif.h&rsquo; <br>
MPI_ALLREDUCE(<i>SENDBUF</i>, <i>RECVBUF</i>, <i>COUNT</i>,
<i>DATATYPE</i>, <i>OP</i>,</p>

<table width="100%" border=0 rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="8%"></td>
<td width="7%">
</td>
<td width="8%">
</td>
<td width="8%">


<p valign="top"><i>COMM</i>, <i>IERROR</i>)</p></td>
<td width="69%">
</td>
<tr valign="top" align="left">
<td width="8%"></td>
<td width="7%">
</td>
<td width="8%">


<p valign="top">&lt;type&gt;</p></td>
<td width="8%"></td>
<td width="69%">


<p valign="top"><i>SENDBUF</i>(*), <i>RECVBUF</i>(*)</p></td>
<tr valign="top" align="left">
<td width="8%"></td>
<td width="7%">
</td>
<td width="8%">


<p valign="top">INTEGER</p></td>
<td width="8%"></td>
<td width="69%">


<p valign="top"><i>COUNT</i>, <i>DATATYPE</i>, <i>OP</i>,
<i>COMM</i>, <i>IERROR</i></p></td>
</table>

<a name="C++ Syntax"></a>
<h2>C++ Syntax</h2>


<p style="margin-left:11%; margin-top: 1em">#include
&lt;mpi.h&gt; <br>
void MPI::Comm::Allreduce(const void* <i>sendbuf</i>, void*
<i>recvbuf</i>,</p>

<table width="100%" border=0 rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="8%"></td>
<td width="92%">


<p valign="top">int <i>count</i>, const MPI::Datatype&amp;
<i>datatype</i>, const</p></td>
<tr valign="top" align="left">
<td width="8%"></td>
<td width="92%">


<p valign="top">MPI::Op&amp; <i>op</i>) const=0</p></td>
</table>

<a name="INPUT PARAMETERS"></a>
<h2>INPUT PARAMETERS</h2>


<table width="100%" border=0 rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="12%">


<p style="margin-top: 1em" valign="top">sendbuf</p></td>
<td width="3%"></td>
<td width="69%">


<p style="margin-top: 1em" valign="top">Starting address of
send buffer (choice).</p></td>
<td width="5%">
</td>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="12%">


<p style="margin-top: 1em" valign="top">count</p></td>
<td width="3%"></td>
<td width="69%">


<p style="margin-top: 1em" valign="top">Number of elements
in send buffer (integer).</p></td>
<td width="5%">
</td>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="12%">


<p style="margin-top: 1em" valign="top">datatype</p></td>
<td width="3%"></td>
<td width="69%">


<p style="margin-top: 1em" valign="top">Datatype of
elements of send buffer (handle).</p></td>
<td width="5%">
</td>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="12%">


<p style="margin-top: 1em" valign="top">op</p></td>
<td width="3%"></td>
<td width="69%">


<p style="margin-top: 1em" valign="top">Operation
(handle).</p> </td>
<td width="5%">
</td>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="12%">


<p style="margin-top: 1em" valign="top">comm</p></td>
<td width="3%"></td>
<td width="69%">


<p style="margin-top: 1em" valign="top">Communicator
(handle).</p> </td>
<td width="5%">
</td>
</table>

<a name="OUTPUT PARAMETERS"></a>
<h2>OUTPUT PARAMETERS</h2>


<table width="100%" border=0 rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="11%">


<p style="margin-top: 1em" valign="top">recvbuf</p></td>
<td width="4%"></td>
<td width="68%">


<p style="margin-top: 1em" valign="top">Starting address of
receive buffer (choice).</p></td>
<td width="6%">
</td>
<tr valign="top" align="left">
<td width="11%"></td>
<td width="11%">


<p style="margin-top: 1em" valign="top">IERROR</p></td>
<td width="4%"></td>
<td width="68%">


<p style="margin-top: 1em" valign="top">Fortran only: Error
status (integer).</p></td>
<td width="6%">
</td>
</table>

<a name="DESCRIPTION"></a>
<h2>DESCRIPTION</h2>


<p style="margin-left:11%; margin-top: 1em">Same as
MPI_Reduce except that the result appears in the receive
buffer of all the group members.</p>

<p style="margin-left:11%; margin-top: 1em"><b>Example
1:</b> A routine that computes the product of a vector and
an array that are distributed across a group of processes
and returns the answer at all nodes (compare with Example 2,
with MPI_Reduce, below).</p>

<p style="margin-left:11%; margin-top: 1em">SUBROUTINE
PAR_BLAS2(m, n, a, b, c, comm) <br>
REAL a(m), b(m,n) ! local slice of array <br>
REAL c(n) ! result <br>
REAL sum(n) <br>
INTEGER n, comm, i, j, ierr</p>

<p style="margin-left:11%; margin-top: 1em">! local sum
<br>
DO j= 1, n <br>
sum(j) = 0.0 <br>
DO i = 1, m <br>
sum(j) = sum(j) + a(i)*b(i,j) <br>
END DO <br>
END DO</p>

<p style="margin-left:11%; margin-top: 1em">! global sum
<br>
CALL MPI_ALLREDUCE(sum, c, n, MPI_REAL, MPI_SUM, comm,
ierr)</p>

<p style="margin-left:11%; margin-top: 1em">! return result
at all nodes <br>
RETURN</p>

<p style="margin-left:11%; margin-top: 1em"><b>Example
2:</b> A routine that computes the product of a vector and
an array that are distributed across a group of processes
and returns the answer at node zero.</p>

<p style="margin-left:11%; margin-top: 1em">SUBROUTINE
PAR_BLAS2(m, n, a, b, c, comm) <br>
REAL a(m), b(m,n) ! local slice of array <br>
REAL c(n) ! result <br>
REAL sum(n) <br>
INTEGER n, comm, i, j, ierr</p>

<p style="margin-left:11%; margin-top: 1em">! local sum
<br>
DO j= 1, n <br>
sum(j) = 0.0 <br>
DO i = 1, m <br>
sum(j) = sum(j) + a(i)*b(i,j) <br>
END DO <br>
END DO</p>

<p style="margin-left:11%; margin-top: 1em">! global sum
<br>
CALL MPI_REDUCE(sum, c, n, MPI_REAL, MPI_SUM, 0, comm,
ierr)</p>

<p style="margin-left:11%; margin-top: 1em">! return result
at node zero (and garbage at the other nodes) <br>
RETURN</p>

<a name="USE OF IN-PLACE OPTION"></a>
<h2>USE OF IN-PLACE OPTION</h2>


<p style="margin-left:11%; margin-top: 1em">When the
communicator is an intracommunicator, you can perform an
all-reduce operation in-place (the output buffer is used as
the input buffer). Use the variable MPI_IN_PLACE as the
value of <i>sendbuf</i> at all processes.</p>

<p style="margin-left:11%; margin-top: 1em">Note that
MPI_IN_PLACE is a special kind of value; it has the same
restrictions on its use as MPI_BOTTOM.</p>

<p style="margin-left:11%; margin-top: 1em">Because the
in-place option converts the receive buffer into a
send-and-receive buffer, a Fortran binding that includes
INTENT must mark these as INOUT, not OUT.</p>

<a name="WHEN COMMUNICATOR IS AN INTER-COMMUNICATOR"></a>
<h2>WHEN COMMUNICATOR IS AN INTER-COMMUNICATOR</h2>


<p style="margin-left:11%; margin-top: 1em">When the
communicator is an inter-communicator, the reduce operation
occurs in two phases. The data is reduced from all the
members of the first group and received by all the members
of the second group. Then the data is reduced from all the
members of the second group and received by all the members
of the first. The operation exhibits a symmetric,
full-duplex behavior.</p>

<p style="margin-left:11%; margin-top: 1em">When the
communicator is an intra-communicator, these groups are the
same, and the operation occurs in a single phase.</p>

<a name="NOTES ON COLLECTIVE OPERATIONS"></a>
<h2>NOTES ON COLLECTIVE OPERATIONS</h2>


<p style="margin-left:11%; margin-top: 1em">The reduction
functions ( <i>MPI_Op</i> ) do not return an error value. As
a result, if the functions detect an error, all they can do
is either call <i>MPI_Abort</i> or silently skip the
problem. Thus, if you change the error handler from
<i>MPI_ERRORS_ARE_FATAL</i> to something else, for example,
<i>MPI_ERRORS_RETURN</i> , then no error may be
indicated.</p>

<a name="ERRORS"></a>
<h2>ERRORS</h2>


<p style="margin-left:11%; margin-top: 1em">Almost all MPI
routines return an error value; C routines as the value of
the function and Fortran routines in the last argument. C++
functions do not return errors. If the default error handler
is set to MPI::ERRORS_THROW_EXCEPTIONS, then on error the
C++ exception mechanism will be used to throw an
MPI:Exception object.</p>

<p style="margin-left:11%; margin-top: 1em">Before the
error value is returned, the current MPI error handler is
called. By default, this error handler aborts the MPI job,
except for I/O function errors. The error handler may be
changed with MPI_Comm_set_errhandler; the predefined error
handler MPI_ERRORS_RETURN may be used to cause error values
to be returned. Note that MPI does not guarantee that an MPI
program can continue past an error.</p>
<hr>
<a href="allindex.html">MPI API Index</a></body>
</html>
